{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set feature service variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_srvc = 'https://services3.arcgis.com/0Fs3HcaFfvzXvm7w/ArcGIS/rest/services/CRIS_Zonal_Statistics_by_County/FeatureServer/5'\n",
    "query = '/query'\n",
    "\n",
    "# Use Geographic Identifiers (Field Name: GEOID) to pick which counties to calculate zonal statistics over.\n",
    "uniqueID = 'GEOID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick counties, years, model sets, and models to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the counties you want to process\n",
    "# Put each county's GEOID between apostrophes and separate by commas, e.g. ids = '20201', '20117'\n",
    "# Set ids to 'all' if using all disctrict IDs in the feature layer\n",
    "ids = '20201', '20203', '20205'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the start and end year of the period you want to process\n",
    "# If you only want to process 1 year, put your desired year for both the start and end\n",
    "year_start = 2050\n",
    "year_end = 2060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the model set you want to process\n",
    "# Put each model set between apostrophes and separate by commas, e.g. 'STAR', 'LOCA2'\n",
    "# Set model_sets to 'all' if you want to return all model sets\n",
    "model_sets = 'LOCA2-STAR', 'STAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the specific models or the weighted ensemble of the model set you want to process\n",
    "# Put each model between apostrophes and separate by commas, e.g. ids = '20201', '20117'\n",
    "# NOTE: only for STAR and LOCA2. LOCA2STAR only includes the weighted ensemble\n",
    "# Set models to 'all' if you want to return all models\n",
    "\n",
    "# Choose from:\n",
    "# 'Ensemble',\n",
    "# 'ACCESS-CM2',\n",
    "# 'ACCESS-ESM1-5',\n",
    "# 'BCC-CSM2-MR',\n",
    "# 'CanESM5',\n",
    "# 'EC-Earth3',\n",
    "# 'FGOALS-g3',\n",
    "# 'GFDL-ESM4',\n",
    "# 'INM-CM4-8',\n",
    "# 'INM-CM5-0',\n",
    "# 'IPSL-CM6A-LR',\n",
    "# 'MIROC6',\n",
    "# 'MPI-ESM1-2-HR',\n",
    "# 'MPI-ESM1-2-LR',\n",
    "# 'MRI-ESM2-0',\n",
    "# 'NorESM2-LM',\n",
    "# 'NorESM2-MM'\n",
    "\n",
    "models = 'Ensemble', 'ACCESS-CM2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ids == 'all':\n",
    "    \n",
    "    id_condition = \"\"\n",
    "\n",
    "elif np.size(ids) == 1:\n",
    "\n",
    "    id_condition = f\"{uniqueID} = '{ids}' AND\"\n",
    "\n",
    "elif np.size(ids) > 1:\n",
    "\n",
    "    id_condition = f\"{uniqueID} IN {ids} AND\"\n",
    "\n",
    "\n",
    "if model_sets == 'all':\n",
    "\n",
    "    model_set_condition = \"\"\n",
    "\n",
    "elif np.size(model_sets) == 1:\n",
    "\n",
    "    model_set_condition = f\"AND MODEL_SET = '{model_sets}'\"\n",
    "        \n",
    "elif np.size(model_sets) > 1:\n",
    "    \n",
    "    model_set_condition = f\"AND MODEL_SET IN {model_sets}\"\n",
    "\n",
    "\n",
    "if models == 'all':\n",
    "\n",
    "    model_condition = \"\"\n",
    "\n",
    "elif np.size(models) == 1:\n",
    "\n",
    "    # model_condition = f\"AND MODEL = '{models[0]}'\"\n",
    "    model_condition = f\"AND MODEL = '{models}'\"\n",
    "\n",
    "elif np.size(models) > 1:\n",
    "\n",
    "    model_condition = f\"AND MODEL IN {models}\"    \n",
    "\n",
    "where_clause_id = f\"{id_condition} YEAR >= {year_start} AND YEAR <= {year_end} {model_set_condition} {model_condition}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick variables and SSP scenarios to return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick one of the three options below, run cells accordingly, then skip to \"Retrieve data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Variables names include the SSP scenario as well.\n",
    "##### Note: There is no SSP370 data for STAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 1: return all variables and all SSP scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 2: return manually selected subset of variables and SSP scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available variables\n",
    "response_temp = requests.get(feat_srvc + '?f=pjson')\n",
    "data = response_temp.json()\n",
    "fields = data['fields']\n",
    "for field in fields:\n",
    "    print('Variable:', field['name'], '\\nDescription:', field['alias'], '\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want 1 or more variables from the above list, select them here\n",
    "# Put each variable between apostrophes and separate by commas, e.g. variables = 'TMAX_SSP245_MIN', 'TMIN_SSP585_MEAN'\n",
    "variables = 'TMAX_SSP245_MIN', 'TMAX_SSP245_MEAN', 'TMAX_SSP245_MAX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option 3: return sets of variables.\n",
    "##### For instance, all SSP245 variables or all 'Annual average daily maximum temperature' variables (TMAX_*) for all SSP scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the line below for all SSP245 variables\n",
    "# variables = [field['name'] for field in fields if \"SSP245\" in field['name']]\n",
    "\n",
    "# Run the line below for all SSP245 and 'Annual average daily maximum temperature' variables (TMAX_*) variables\n",
    "variables = [field['name'] for field in fields if \"TMAX_\" in field['name'] and \"SSP245\" in field['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add counties ('GEOID') and 'YEAR', MODEL_SET, and MODEL to the variables-to-return list\n",
    "return_variables = ['GEOID', 'YEAR', 'MODEL_SET', 'MODEL']\n",
    "\n",
    "if np.size(variables) == 1:\n",
    "\n",
    "    return_variables.append(variables)\n",
    "\n",
    "else:\n",
    "\n",
    "    for var in variables:\n",
    "\n",
    "        return_variables.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of variables that can be processed in each request: 50\n",
    "# Maximum number of rows that can be processed in each request: 1000\n",
    "# This code loops through variables and rows and appends each subset to a final GeoDataFrame (gdf)\n",
    "\n",
    "# Number of variables that are requested to be processed\n",
    "vars_to_process = len(return_variables)\n",
    "\n",
    "# Number of variables to process in each loop\n",
    "loop_size = 50\n",
    "\n",
    "# Set initial value for loop number (v)\n",
    "v = 0\n",
    "\n",
    "# Loop through variables, 50 at one time\n",
    "# While there are variables left to process, keep looping through this code\n",
    "while vars_to_process > 0:\n",
    "\n",
    "    # Sequentially subset the variables to process with 50 variables in each loop\n",
    "    return_variables_condition = return_variables[max((loop_size*v - 1) + 1, 0): min(loop_size*(v+1), len(return_variables))]\n",
    "\n",
    "    # Update number of variables to process, i.e. subtract 50 (loop_size)\n",
    "    vars_to_process -= loop_size\n",
    "    \n",
    "    # Update loop number (v)\n",
    "    v += 1\n",
    "\n",
    "    # Set parameters for requests.get()\n",
    "    params = {\n",
    "        'where': where_clause_id,\n",
    "        'outFields': return_variables_condition,\n",
    "        'orderByFields': ['YEAR', 'GEOID'],\n",
    "        'f': 'pgeojson',\n",
    "    }\n",
    "\n",
    "    # Request response from feature service\n",
    "    response = requests.get(feat_srvc + query, params=params)\n",
    "\n",
    "\n",
    "    # Put the first 50 variables in gdf to establish the GeoDataFrame\n",
    "    if v == 1:\n",
    "        \n",
    "        # Translate the reponse.text into a temporary GeoDataFrame\n",
    "        gdf_var1_temp = gpd.read_file(response.text)\n",
    "\n",
    "        # Drop geometry field\n",
    "        gdf_var1_temp.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "        # Establish the final GeoDataFrame (gdf) and put the first subset of rows and variables (temporary GeoDataFrame) in it\n",
    "        gdf = gdf_var1_temp\n",
    "\n",
    "\n",
    "        # Set initial value for loop number (i)\n",
    "        i = 0\n",
    "\n",
    "        # Loop through rows, 1000 at one time\n",
    "        while len(gdf_var1_temp) == 1000:\n",
    "            \n",
    "            # Update loop number (i)\n",
    "            i = i + 1\n",
    "\n",
    "            # Set the offset for in params. This says how many rows to skip from the start.\n",
    "            offset = 1000 * i\n",
    "\n",
    "            # Set parameters for requests.get()\n",
    "            params = {\n",
    "                'where': where_clause_id,\n",
    "                'outFields': return_variables_condition,\n",
    "                'orderByFields': ['YEAR', 'GEOID'],\n",
    "                'f': 'pgeojson',\n",
    "                'resultOffset': f'{offset}'\n",
    "            }\n",
    "\n",
    "            # Request response from feature service\n",
    "            response = requests.get(feat_srvc + query, params=params)\n",
    "\n",
    "            # Translate the reponse.text into a temporary GeoDataFrame\n",
    "            gdf_var1_temp = gpd.read_file(response.text)\n",
    "\n",
    "            # Concatenate the new subset to the final GeoDataFrame (gdf)\n",
    "            gdf = pd.concat([gdf, gdf_var1_temp], ignore_index=True)\n",
    "\n",
    "            # Drop geometry field\n",
    "            gdf.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "\n",
    "    # Put all the additional variables, beyond the first 50, in a temporary GeoDataFrame and concatenate with the final GeoDataFrame (gdf)\n",
    "    elif v > 1:\n",
    "    \n",
    "        # Translate the reponse.text into a temporary GeoDataFrame\n",
    "        gdf_var2_temp = gpd.read_file(response.text)\n",
    "        \n",
    "        # Drop geometry field\n",
    "        gdf_var2_temp.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "        # Establish the a temporart GeoDataFrame (gdf_var2) and put the first subset of rows and variables (temporary GeoDataFrame) in it     \n",
    "        gdf_var2 = gdf_var2_temp\n",
    "\n",
    "\n",
    "        # Set initial value for loop number (j)\n",
    "        j = 0\n",
    "\n",
    "        # Loop through rows, 1000 at one time\n",
    "        while len(gdf_var2_temp) == 1000:\n",
    "\n",
    "            # Update loop number (j)\n",
    "            j = j + 1\n",
    "\n",
    "            # Set the offset for in params. This says how many rows to skip from the start.\n",
    "            offset = 1000 * j\n",
    "\n",
    "            # Set parameters for requests.get()\n",
    "            params = {\n",
    "                'where': where_clause_id,\n",
    "                'outFields': return_variables_condition,\n",
    "                'orderByFields': ['YEAR', 'GEOID'],\n",
    "                'f': 'pgeojson',\n",
    "                'resultOffset': f'{offset}'\n",
    "            }\n",
    "\n",
    "            # Request response from feature service\n",
    "            response = requests.get(feat_srvc + query, params=params)\n",
    "\n",
    "            # Translate the reponse.text into a temporary GeoDataFrame\n",
    "            gdf_var2_temp = gpd.read_file(response.text)\n",
    "\n",
    "            # Concatenate the new subset to the temporary GeoDataFrame (gdf_var2)\n",
    "            gdf_var2 = pd.concat([gdf_var2, gdf_var2_temp], ignore_index=True)\n",
    "\n",
    "            # Drop geometry field\n",
    "            gdf_var2.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "        # Concatenate temporary GeoDataFrame (gdf_var2) to the final GeoDataFrame (gdf)\n",
    "        gdf = pd.concat([gdf, gdf_var2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation 1: mean over time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, run this cell to take a subset of the time period over which to calculate the mean\n",
    "year_start_subset = 2054\n",
    "year_end_subset = 2056\n",
    "\n",
    "gdf = gdf[(gdf['YEAR'] >= year_start_subset) & (gdf['YEAR'] < year_end_subset+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run this cell to return mean of each variable in the GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by county (GEOID), MODEL_SET, and MODEL and calculate mean of period 'year_start_subset - year_end_subset'\n",
    "variables_mean = gdf.groupby(['GEOID', 'MODEL_SET', 'MODEL']).mean()\n",
    "\n",
    "# Drop YEAR column\n",
    "variables_mean.drop(columns=['YEAR'], inplace=True)\n",
    "\n",
    "# If returning all variables, remove OBJECTID and BATCH_ID columns\n",
    "if variables == '*':\n",
    "    variables_mean.drop(columns=['OBJECTID', 'BATCH_ID'], inplace=True)\n",
    "\n",
    "# Add index numbers to rows\n",
    "variables_mean.reset_index(inplace=True)\n",
    "\n",
    "# Rename column names to reflect that they represent the mean of each variable over time\n",
    "variables_mean.rename(columns={gdf.columns[i]: f\"{gdf.columns[i]} (mean)\" for i in range(1, len(gdf.columns))}, inplace=True)\n",
    "\n",
    "# Show means of variables\n",
    "variables_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation 2: change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set years over which to calculate the change\n",
    "year_start_change = 2052\n",
    "year_end_change = 2058\n",
    "\n",
    "# Create two new GeoDataFrames which contain only the the start and end years, respectively\n",
    "gdf_start_change = gdf[(gdf['YEAR'] == year_start_change)]\n",
    "gdf_end_change = gdf[(gdf['YEAR'] == year_end_change)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the indices of both GeoDataFrames to the counties (GEOIDs)\n",
    "gdf_start_change.set_index('GEOID', inplace=True);\n",
    "gdf_end_change.set_index('GEOID', inplace=True);\n",
    "\n",
    "# Drop YEAR, MODEL_SET, and MODEL columns\n",
    "gdf_start_change.drop(columns=['YEAR'], inplace=True)\n",
    "gdf_end_change.drop(columns=['YEAR'], inplace=True)\n",
    "\n",
    "# If returning all variables, remove OBJECTID and BATCH_ID columns\n",
    "if variables == '*':\n",
    "    gdf_start_change.drop(columns=['OBJECTID', 'BATCH_ID'], inplace=True)\n",
    "    gdf_end_change.drop(columns=['OBJECTID', 'BATCH_ID'], inplace=True)\n",
    "\n",
    "gdf_start_change.sort_values(by=['GEOID', 'MODEL_SET', 'MODEL'], inplace=True)\n",
    "gdf_end_change.sort_values(by=['GEOID', 'MODEL_SET', 'MODEL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdf_start_change and gdf_end_change and rename the variable columns by adding _start or _end, respectively\n",
    "gdf_merged_change_absolute = gdf_start_change.merge(gdf_end_change, on=['GEOID', 'MODEL_SET', 'MODEL'], suffixes=('_start', '_end'))\n",
    "gdf_merged_change_percentage = gdf_start_change.merge(gdf_end_change, on=['GEOID', 'MODEL_SET', 'MODEL'], suffixes=('_start', '_end'))\n",
    "\n",
    "# Number of variables (or unique columns) for which to calculate the difference. Subtract 2 for MODEL_SET and MODEL. Divide by 2 because each variable has a start and end column\n",
    "num_columns = int((np.shape(gdf_merged_change_absolute)[1] - 2) / 2)\n",
    "\n",
    "# Loop through variables. Start at 2 to skip MODEL_SET and MODEL\n",
    "for c in range(2, num_columns + 2):\n",
    "\n",
    "    # Calculate difference (= end value - start value), for each variable, MODEL_SET, and MODEL. \"c+num_columns\" is to start at the end columns\n",
    "    gdf_merged_change_absolute[f\"{gdf_merged_change_absolute.columns[c].replace('_start', '')} (change)\"] = gdf_merged_change_absolute[f\"{gdf_merged_change_absolute.columns[c+num_columns]}\"] - gdf_merged_change_absolute[f\"{gdf_merged_change_absolute.columns[c]}\"]\n",
    "    gdf_merged_change_percentage[f\"{gdf_merged_change_percentage.columns[c].replace('_start', '')} (% change)\"] = ((gdf_merged_change_percentage[f\"{gdf_merged_change_percentage.columns[c+num_columns]}\"] - gdf_merged_change_percentage[f\"{gdf_merged_change_percentage.columns[c]}\"]) / gdf_merged_change_percentage[f\"{gdf_merged_change_percentage.columns[c]}\"]) * 100\n",
    "    \n",
    "\n",
    "# Drop the _start and _end columns. Only keep the difference columns\n",
    "drop_columns = [col for col in gdf_merged_change_absolute.columns if '_start' in col or '_end' in col]\n",
    "variables_difference_absolute = gdf_merged_change_absolute.drop(columns=drop_columns)\n",
    "variables_difference_percentage = gdf_merged_change_percentage.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show absolute difference of each variable between the start and end years\n",
    "variables_difference_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentage difference of each variable between the start and end years\n",
    "variables_difference_percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
