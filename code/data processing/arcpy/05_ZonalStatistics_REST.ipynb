{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate zonal statistics\n",
    "\n",
    "This is the fifth notebook in a series of six notebooks designed to run sequentially. \\\n",
    "Run this notebook to calculate zonal statistics (mean) for each model set-model combination and for each multimodel weighted ensemble. \\\n",
    "This notebook uses REST services for the zone features. \\\n",
    "Insert your desired feature service URL below under 'Your feature service'. \\\n",
    "Example zones are included for counties, watershed boundaries, and tribal areas for CONUS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "from arcgis.gis import GIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your username and password for the GIS connection\n",
    "gis = GIS('https://nationalclimate.maps.arcgis.com/', username='*****', password='*****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_folder(name):\n",
    "\n",
    "    if arcpy.Exists(name):\n",
    "    \n",
    "        shutil.rmtree(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(name):    \n",
    "\n",
    "    files = glob.glob(os.path.join(name + '*'))\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        if file in glob.glob(os.path.join(name + '*.lock')):\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            \n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your base directory\n",
    "base = r'C:\\<your-base-path>\\CRIS'\n",
    "out_path = os.path.join(base, 'zonalstats_REST')\n",
    "\n",
    "# Set the workspace and overwrite output\n",
    "arcpy.env.workspace = out_path\n",
    "arcpy.env.overwriteOutput = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 'STAR', 'LOCA2', 'STAR_ensemble', 'LOCA2_ensemble', or 'LOCA2STAR_ensemble'\n",
    "model_set_start = 'LOCA2STAR_ensemble'\n",
    "\n",
    "# pick model (only for STAR and LOCA2)\n",
    "model = 'ACCESS-CM2'\n",
    "\n",
    "# Pick one or more variables\n",
    "variables = ['cdd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select SSP Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp = '245'\n",
    "# ssp = '585'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick start and end year of the period you want to process.\n",
    "# If you only want to process 1 year, put your desired year for both the start and end.\n",
    "\n",
    "year_start = 2040\n",
    "year_end = 2040\n",
    "\n",
    "time_start = f'{year_start}-12-31T00:00:00'\n",
    "time_end = f'{year_end}-12-31T00:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Service. Only run the cell with your preferred feature layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USA 119th Congressional Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_srvc = 'https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_119th_Congressional_Districts/FeatureServer/0'\n",
    "\n",
    "feat_name = feat_srvc.split('/')[-3]\n",
    "\n",
    "# Use a uniquely defining field to pick which USA Congressional District(s) to calculate zonal statistics over.\n",
    "# In this case, use district IDs (Field Name: DISTRICTID).\n",
    "uniqueID = 'DISTRICTID'\n",
    "\n",
    "# Put each district ID between apostrophes and separate by commas.\n",
    "# Set ids to 'all' if using all disctrict IDs in the feature layer\n",
    "ids = '0634', '0635', '0636', '0101', '0102'\n",
    "# ids = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USA ZIP Code Boundaries 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_srvc = 'https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/USA_Boundaries_2023/FeatureServer/3'\n",
    "\n",
    "feat_name = feat_srvc.split('/')[-3]\n",
    "\n",
    "# Use a uniquely defining field to pick which ZIP code(s) to calculate zonal statistics over.\n",
    "# In this case, use the base names, i.e. ZIP codes (Field Name: BASENAME).\n",
    "# uniqueID = 'BASENAME'\n",
    "uniqueID = 'ZIP_CODE'\n",
    "\n",
    "# Put each ZIP code between apostrophes and separate by commas.\n",
    "# Set ids to 'all' if using all ZIP codes in the feature layer\n",
    "ids = '90025', '90026', '90027', '90028', '90029', '90030', '90031'\n",
    "# ids = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Watershed Boundary Dataset HUC 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_srvc = 'https://services.arcgis.com/P3ePLMYs2RVChkJx/arcgis/rest/services/Watershed_Boundary_Dataset_HUC_6s/FeatureServer/0'\n",
    "\n",
    "feat_name = feat_srvc.split('/')[-3]\n",
    "\n",
    "# Use a uniquely defining field to pick which watershed boundaries to calculate zonal statistics over.\n",
    "# In this case, use the watershed boundary name (Field Name: NAME).\n",
    "uniqueID = 'NAME'\n",
    "\n",
    "# Put each watershed boundary name between apostrophes and separate by commas.\n",
    "# Set ids to 'all' if using all watershed boundaries in the feature layer\n",
    "ids = 'Santa Ana', 'Wabash', 'Santa Cruz', 'Southern Mojave'\n",
    "# ids = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Your feature service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_srvc = '<your-feature-service-url>' # don't forget to add your layer number at the end of the url, e.g. '/0'\n",
    "\n",
    "feat_name = feat_srvc.split('/')[-3]\n",
    "\n",
    "# Use a uniquely defining field to pick which feature(s) to calculate zonal statistics over.\n",
    "uniqueID = ''\n",
    "\n",
    "# Put each ID between apostrophes and separate by commas.\n",
    "# Set ids to 'all' if using all features in the feature layer\n",
    "ids = ''\n",
    "# ids = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define (subset of) feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ids == 'all':\n",
    "    \n",
    "    feat_layer = feat_srvc\n",
    "\n",
    "else:\n",
    "    \n",
    "    if np.size(ids) == 1:\n",
    "\n",
    "        where_clause_id = f\"{uniqueID} = '{ids}'\"\n",
    "        feat_layer = arcpy.management.MakeFeatureLayer(feat_srvc, 'feat_layer', where_clause_id)\n",
    "\n",
    "    else:\n",
    "\n",
    "        where_clause_id = f\"{uniqueID} IN {ids}\"\n",
    "        feat_layer = arcpy.management.MakeFeatureLayer(feat_srvc, 'feat_layer', where_clause_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variables:\n",
    "    \n",
    "    print('Variable being processed:', var)\n",
    "    print('Setting variable and path names')\n",
    "\n",
    "    # Create output geodatabase names\n",
    "    gdb_name_temp = feat_name + '_temp.gdb'\n",
    "    gdb_name = feat_name + '.gdb'\n",
    "\n",
    "    # Create output geodatabase paths\n",
    "    out_gdb_path_temp = os.path.join(out_path, gdb_name_temp)\n",
    "    out_gdb_path = os.path.join(out_path, gdb_name)\n",
    "    \n",
    "    # Create output geodatabases if they do not exist\n",
    "    if not arcpy.Exists(out_gdb_path_temp):\n",
    "        arcpy.management.CreateFileGDB(out_path, gdb_name_temp)\n",
    "    \n",
    "    if not arcpy.Exists(out_gdb_path):\n",
    "        arcpy.management.CreateFileGDB(out_path, gdb_name)\n",
    "    \n",
    "    # Change variable name to a format that is compatible with file names\n",
    "    var_format = var.replace('-','_').replace(' ', '_').replace(',','').replace('(','')\n",
    "\n",
    "    # Set output file and geodatabase table names\n",
    "    crf_out = os.path.join(out_path, var_format + '.crf')\n",
    "    out_gdb_table = os.path.join(out_gdb_path_temp, var_format + f'_{year_start}_{year_end}')    \n",
    "    \n",
    "    # Create path names for STAR and LOCA2 model sets\n",
    "    if model_set_start == 'STAR' or model_set_start =='LOCA2':\n",
    "\n",
    "        # Create path name based on model_set_start and variable\n",
    "        netcdf_dir_list = os.path.join(base, 'data', 'resample_mask', model_set_start, var)\n",
    "\n",
    "        # Create netcdf name based on model_set_start, model, variable, and ssp\n",
    "        netcdf_name = os.path.join(netcdf_dir_list, f'{model_set_start}_{model}_{var_format}_ssp{ssp}_1950_2100.nc')\n",
    "\n",
    "    # Create path names for STAR_ensemble, LOCA2_ensemble, and LOCA2STAR_ensemble model sets\n",
    "    elif model_set_start == 'STAR_ensemble' or model_set_start == 'LOCA2_ensemble' or model_set_start == 'LOCA2STAR_ensemble':\n",
    "\n",
    "        # Remove '_ensemble' from model_set_start\n",
    "        model_set_start_format = model_set_start.strip('_ensemble')\n",
    "\n",
    "        # Create path name based on model_set_start\n",
    "        netcdf_dir_list = os.path.join(base, 'data', 'ensemble', model_set_start_format)\n",
    "\n",
    "        # Create netcdf name based on model_set_start, variable, and ssp\n",
    "        netcdf_name = os.path.join(netcdf_dir_list, f'{model_set_start_format}_{var_format}_ssp{ssp}_1950_2100.nc')\n",
    "\n",
    "    # Strip .nc from the netcdf_name and add .crf\n",
    "    crf_name = f'{model_set_start_format}_{var_format}_ssp{ssp}_1950_2100.crf'\n",
    "\n",
    "    # Copy the netcdf file to a crf file with the same name\n",
    "    print('Processing: CopyRaster')\n",
    "    arcpy.management.CopyRaster(netcdf_name, os.path.join(out_path, crf_name), process_as_multidimensional='ALL_SLICES', build_multidimensional_transpose='TRANSPOSE')\n",
    "\n",
    "    # Use the copied crf file as the input for SubsetMultidimensionalRaster\n",
    "    img_srvc = os.path.join(out_path, crf_name)\n",
    "\n",
    "    # Subset variable and time\n",
    "    print('Processing: SubsetMultidimensionalRaster 1')\n",
    "    arcpy.md.SubsetMultidimensionalRaster(img_srvc,\n",
    "                                          var_format,\n",
    "                                          variables=var,\n",
    "                                          dimension_def = 'BY_RANGES',\n",
    "                                          dimension_ranges = 'StdTime ' + time_start + ' ' + time_end\n",
    "                                          )\n",
    "\n",
    "    # Calculate Zonal Stastics\n",
    "    print('Processing: ZonalStatisticsAsTable 1')\n",
    "    arcpy.ia.ZonalStatisticsAsTable(feat_layer, uniqueID, crf_out, out_gdb_table, ignore_nodata='DATA', statistics_type='MEAN', process_as_multidimensional='ALL_SLICES')\n",
    "\n",
    "    # Some features can be very small and might not be captured by the first Zonal Statistics As Table. These need to be converted to points and then re-processed.\n",
    "    # Check if number of features in the output gdb is equal to the number of features in the input feature layer\n",
    "    # If not, the below code (after \"elif\") will add the missing features to the output gdb\n",
    "    if int(arcpy.management.GetCount(feat_layer).getOutput(0)) == int(arcpy.management.GetCount(out_gdb_table).getOutput(0)):\n",
    "\n",
    "        # Don't sort if there is only one feature in out_gdb_table\n",
    "        if int(arcpy.management.GetCount(out_gdb_table).getOutput(0)) != 1:\n",
    "\n",
    "            # Sort the zonal statistics table\n",
    "            print('Processing: Sort')\n",
    "            output_all_sorted = os.path.join(out_gdb_path, var_format + f'_SSP{ssp}' + f'_{year_start}_{year_end}')\n",
    "            arcpy.management.Sort(out_gdb_table, output_all_sorted, [[uniqueID, 'ASCENDING']]);\n",
    "\n",
    "    elif int(arcpy.management.GetCount(feat_layer).getOutput(0)) != int(arcpy.management.GetCount(out_gdb_table).getOutput(0)):\n",
    "\n",
    "        # Raster to Point using your input value raster (this raster has the desired output spatial resolution)\n",
    "        print('Processing: RasterToPoint')\n",
    "        arcpy.conversion.RasterToPoint(crf_out, os.path.join(out_path, 'RtoP.shp'), 'Value');\n",
    "\n",
    "        # Spatial Join single part features from #2 (Join Features) to the grid points from #1 (Target Features).\n",
    "        # Join One-to-one & uncheck Keep all target features.\n",
    "        # Unchecking the Keep all target features results in some centroid points being removed from the output if they do not intersect with a polygon.\n",
    "        print('Processing: SpatialJoin')\n",
    "        arcpy.analysis.SpatialJoin(os.path.join(out_path, 'RtoP'), feat_layer, os.path.join(out_path, 'features_and_RtoP'), 'JOIN_ONE_TO_ONE', 'KEEP_COMMON');\n",
    "\n",
    "        # Select by Location single part features from #2 that do NOT intersect results from #3.\n",
    "        print('Processing: SelectLayerByLocation')\n",
    "        feat_layer_select_location = arcpy.management.SelectLayerByLocation(feat_layer, 'INTERSECT', os.path.join(out_path, 'features_and_RtoP'), invert_spatial_relationship='INVERT');\n",
    "\n",
    "        # Feature to Point the results from #4. Check Inside.\n",
    "        print('Processing: FeatureToPoint')\n",
    "        arcpy.management.FeatureToPoint(feat_layer_select_location, os.path.join(out_path, 'feat_layer_FtoP'), point_location='INSIDE');\n",
    "\n",
    "        # Do zonalstats for extra points not captured by the first Zonal Statistics As Table\n",
    "        print('Processing: ZonalStatisticsAsTable 2')\n",
    "        temp_table = os.path.join(out_gdb_path_temp, var_format + f'_SSP{ssp}' + f'_{year_start}_{year_end}_added_points_pre_merge')\n",
    "        arcpy.ia.ZonalStatisticsAsTable(os.path.join(out_path, 'feat_layer_FtoP'), uniqueID, crf_out, temp_table, ignore_nodata='DATA', statistics_type='MEAN', process_as_multidimensional='ALL_SLICES');\n",
    "\n",
    "        # merge two zonalstatistics (features and points)\n",
    "        print('Processing: Merge')\n",
    "        input_most = out_gdb_table\n",
    "        input_few = temp_table\n",
    "        output_all = os.path.join(out_gdb_path_temp, var_format + f'_SSP{ssp}' + f'_{year_start}_{year_end}_merge')\n",
    "        arcpy.management.Merge([input_most, input_few], output_all);\n",
    "\n",
    "        # sort the merged zonalstatistics\n",
    "        print('Processing: Sort')\n",
    "        output_all_sorted = os.path.join(out_gdb_path, var_format + f'_SSP{ssp}' + f'_{year_start}_{year_end}')\n",
    "        arcpy.management.Sort(output_all, output_all_sorted, [[uniqueID, 'ASCENDING']]);\n",
    "\n",
    "    # Remove all temporary folders and files\n",
    "    print('Removing all temporary folders and files')\n",
    "    remove_folder(crf_out)\n",
    "    remove_file(os.path.join(out_path,'RtoP'))\n",
    "    remove_file(os.path.join(out_path,'feat_layer_FtoP'))\n",
    "    remove_file(os.path.join(out_path,'features_and_RtoP'))\n",
    "    arcpy.management.Delete(out_gdb_path_temp);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-raf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
