{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate zonal statistics\n",
    "\n",
    "This is the fifth script in a series of six scripts designed to run sequentially. \\\n",
    "Run this script to calculate zonal statistics (mean) for each model set-model combination and for each multimodel weighted ensemble. \\\n",
    "This script uses a local file for the zone features, including counties, watershed boundaries, and tribal areas for CONUS. \\\n",
    "Download the zones (CRIS_Zonal_Stat_Zones_CONUS_4326.gdb) from https://cris.climate.gov/datasets/93ed19088971459f952b598a08ab0ec2/about to <your-base-path>/zonalstats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import traceback\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    \"\"\"\n",
    "    Checks if directory exists and if not it will create it\n",
    "    \n",
    "    dir: str\n",
    "        full path the directory\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir):\n",
    "            os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_return_list_by_selection(workspace, ssp, model):\n",
    "    \"\"\"\n",
    "    Lists all files in a workspace and return the full path\n",
    "\n",
    "    workspace: str\n",
    "        full path the directory\n",
    "    selection: str\n",
    "        string used as a filter \n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(workspace):\n",
    "        if ssp in file and model in file:\n",
    "            files.append(os.path.join(workspace,file))\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_tiff(netcdf, variable, t, temp_dir):\n",
    "\n",
    "    # Define the path to the raster file\n",
    "    ds = xr.open_dataset(netcdf)\n",
    "    data = ds[variable.replace('-','_')].isel(time=t)\n",
    "\n",
    "    # Get the spatial dimensions and resolution\n",
    "    lon = ds['lon'].values - 360\n",
    "    lat = ds['lat'].values\n",
    "    xsize = (lon[1] - lon[0])\n",
    "    ysize = -(lat[1] - lat[0])\n",
    "\n",
    "    year = t + 1950\n",
    "\n",
    "    # shift lon.min() and lat.max() by 0.5*pixel size\n",
    "    lonmin = lon.min() - xsize/2\n",
    "    latmax = lat.max() + ysize/2\n",
    "\n",
    "    transform = from_origin(lonmin, latmax, xsize, ysize)\n",
    "    \n",
    "    output_tiff = os.path.join(temp_dir, os.path.basename(netcdf)).strip('.nc') + '_temp_' + str(year) + '.tif'\n",
    "\n",
    "    # Create an in-memory raster layer using rasterio\n",
    "    with rasterio.open(\n",
    "                    output_tiff,\n",
    "                    'w',\n",
    "                    driver='GTiff',\n",
    "                    height=data.shape[0],\n",
    "                    width=data.shape[1],\n",
    "                    count=1,\n",
    "                    dtype=data.dtype,\n",
    "                    crs='+proj=latlong',\n",
    "                    transform=transform,\n",
    "    ) as dst:\n",
    "                    dst.write(np.flipud(data.values), 1)\n",
    "\n",
    "    return dst, output_tiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Logger and Open Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your base directory\n",
    "base = r'C:\\<your-base-path>\\CRIS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE_PATH = os.path.join(base, 'logs')\n",
    "\n",
    "FILE_TIME = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_FILE_NAME = \"Output_Log_zonalstats_first_\" + FILE_TIME + \".log\"\n",
    "LOG_FILE = os.path.join(LOG_FILE_PATH, LOG_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(level = logging.INFO,\n",
    "                    format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "                    handlers=[\n",
    "                       logging.FileHandler(filename=LOG_FILE),\n",
    "                       logging.StreamHandler()\n",
    "                   ])\n",
    "logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the zones: CONUS Counties, Hydrologic Unit Codes (HUC8), and Tribal Areas.\n",
    "# Download the zones (CRIS_Zonal_Stat_Zones_CONUS_4326.gdb) from https://cris.climate.gov/datasets/93ed19088971459f952b598a08ab0ec2/about to <your-base-path>/zonalstats\n",
    "input_zone = os.path.join(base, 'zonalstats\\CRIS_Zonal_Stat_Zones_CONUS_4326.gdb')\n",
    "\n",
    "input_dir = os.path.join(base, 'data')\n",
    "output_dir = os.path.join(base, 'zonalstats', 'out')\n",
    "temp_dir = os.path.join(base, 'zonalstats', 'temp')\n",
    "\n",
    "create_dir(temp_dir)\n",
    "create_dir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 'STAR', 'LOCA2', 'STAR_ensemble', 'LOCA2_ensemble', or 'LOCA2STAR_ensemble'\n",
    "model_set_start = 'LOCA2STAR_ensemble'\n",
    "\n",
    "# pick model (only for STAR and LOCA2)\n",
    "model = 'ACCESS-CM2'\n",
    "\n",
    "# pick variable\n",
    "variable = 'tavg'\n",
    "\n",
    "ssp_list = ['_ssp585'] #['_ssp245','_ssp370','_ssp585']\n",
    "\n",
    "proc_list = ['HUC8'] #['TribalAreas', 'Counties', 'HUC8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cutoff year between historical (1950-2014) and projection (2015-2100)\n",
    "cutoff_year = 2015 - 1950\n",
    "\n",
    "for proc in proc_list:\n",
    "\n",
    "    # Load the zone data from the GDB file\n",
    "    zone_data = gpd.read_file(input_zone, layer=proc)\n",
    "    \n",
    "    # Remove column 'Shape_Length'\n",
    "    if proc == 'Counties':\n",
    "        zone_data = zone_data.drop(columns=['SHAPE_Length'])\n",
    "    elif proc == 'TribalAreas' or proc == 'HUC8':\n",
    "        zone_data = zone_data.drop(columns=['Shape_Length'])\n",
    "\n",
    "    # Define the number of rows per value\n",
    "    num_rows = len(zone_data)\n",
    "    \n",
    "    # Repeat zone_data for each year\n",
    "    zone_data = pd.concat([zone_data] * 151, ignore_index=True)\n",
    "\n",
    "    # Generate year values\n",
    "    values = np.concatenate([np.full(num_rows, value) for value in range(1950, 2101)])\n",
    "\n",
    "    # Create geodataframe of years\n",
    "    repeated_years = gpd.GeoDataFrame({'Standard Time': values})\n",
    "\n",
    "    # Add years to zone_data\n",
    "    zone_data['Standard Time'] = repeated_years['Standard Time']\n",
    "    \n",
    "    # Add empty statistics to zone_data\n",
    "    stat_columns = gpd.GeoDataFrame(index=range(num_rows*151))\n",
    "    stat_columns['MEAN'] = None\n",
    "    zone_data['MEAN'] = stat_columns['MEAN']\n",
    "\n",
    "    # Change column name 'Shape_Area' to 'AREA'\n",
    "    zone_data = zone_data.rename(columns={'Shape_Area': 'AREA'})\n",
    "\n",
    "    # Create output directories\n",
    "    output_dir_model_set = os.path.join(output_dir, model_set_start)\n",
    "    create_dir(output_dir_model_set)\n",
    "\n",
    "    output_proc = os.path.join(output_dir_model_set, proc)\n",
    "    create_dir(output_proc)\n",
    "\n",
    "    # Create directory paths\n",
    "    if model_set_start == 'STAR' or model_set_start =='LOCA2':\n",
    "\n",
    "        netcdf_dir_list = os.path.join(input_dir, 'resample_mask', model_set_start, variable)\n",
    "\n",
    "    elif model_set_start == 'STAR_ensemble' or model_set_start == 'LOCA2_ensemble' or model_set_start == 'LOCA2STAR_ensemble':\n",
    "\n",
    "        netcdf_dir_list = os.path.join(input_dir, 'ensemble', model_set_start.strip('_ensemble'))\n",
    "\n",
    "    # Execute the zonal statistics workflow by ssp in a for loop\n",
    "    for ssp in ssp_list:\n",
    "\n",
    "        # List all netcdf files in the directory\n",
    "        if model_set_start == 'STAR' or model_set_start =='LOCA2':\n",
    "\n",
    "            ssp_rasters = list_dir_return_list_by_selection(netcdf_dir_list, ssp, model)\n",
    "\n",
    "        elif model_set_start == 'STAR_ensemble' or model_set_start == 'LOCA2_ensemble' or model_set_start == 'LOCA2STAR_ensemble':\n",
    "\n",
    "            ssp_rasters = list_dir_return_list_by_selection(netcdf_dir_list, ssp, '')\n",
    "\n",
    "        # Strip .xmls from the ssp_rasters list\n",
    "        ssp_rasters = list(set([str(item).strip(\".xml\") for item in ssp_rasters]))\n",
    "        total_recs = len(ssp_rasters)\n",
    "        logging.info(f\"There are [{total_recs}] files in SSP[{ssp.strip('_ssp')}]\")\n",
    "        rec_count = 0\n",
    "\n",
    "        start_rec_tm = time.time()\n",
    "        \n",
    "        for netcdf in ssp_rasters:\n",
    "\n",
    "            rec_count += 1\n",
    "            start_rec_tm = time.time()\n",
    "\n",
    "            # Create path to save output to\n",
    "            out_raster_path = os.path.join(output_dir, model_set_start, variable, os.path.basename(netcdf))\n",
    "\n",
    "            # Split the file name by underscore and collect the parts for processing\n",
    "            file_name_parts = os.path.basename(netcdf).strip('.nc').split(\"_\")\n",
    "\n",
    "            if model_set_start == 'STAR' or model_set_start =='LOCA2':\n",
    "\n",
    "                # Collect file names parts\n",
    "                model_set, model, variable, ssp, start_year, end_year = file_name_parts\n",
    "                logging.info(f\"Processing {os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp]))}\")\n",
    "\n",
    "                raster_name = '_'.join([model_set, model, variable, ssp.strip('_')]).replace('-','')\n",
    "\n",
    "            elif model_set_start == 'STAR_ensemble' or model_set_start == 'LOCA2_ensemble' or model_set_start == 'LOCA2STAR_ensemble':\n",
    "\n",
    "                # Collect file names parts\n",
    "                model_set, variable, ssp, start_year, end_year = file_name_parts\n",
    "                logging.info(f\"Processing {os.path.join(netcdf_dir_list, '_'.join([variable,ssp]))}\")\n",
    "\n",
    "                raster_name = '_'.join([model_set_start, variable, ssp.strip('_')]).replace('-','')\n",
    "\n",
    "            # Add column for variable\n",
    "            zone_data['Variable'] = variable.replace('-','_')\n",
    "\n",
    "            # Change order of columns\n",
    "            zone_data = zone_data.reindex(columns=['GEOID', 'AREA', 'Variable', 'Standard Time', 'MEAN', 'geometry'])\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            try:\n",
    "                \n",
    "                # Loop over years\n",
    "                for year in range(1950,2101):\n",
    "\n",
    "                    print(year)\n",
    "                    t = year-1950\n",
    "                    \n",
    "                    # Load the zone data geometries from the GDB file\n",
    "                    zone_data_temp = gpd.read_file(input_zone, layer=proc)\n",
    "                    zone_data_geometry = zone_data_temp.geometry\n",
    "            \n",
    "                    # Convert the netcdf to tiff\n",
    "                    dst, output_tiff = netcdf_to_tiff(netcdf, variable, t, temp_dir)\n",
    "\n",
    "                    # Calculate zonal statistics\n",
    "                    stats = zonal_stats(zone_data_geometry, dst.name, stats=['mean'], geojson_out=False)\n",
    "                    \n",
    "                    # Add the statistics to the GeoDataFrame\n",
    "                    zone_data.loc[t*num_rows : (t+1)*num_rows - 1, 'MEAN'] = [stat['mean'] for stat in stats]\n",
    "                    \n",
    "                    # Delete temporary tiff file so there is no buildup of old tiffs that are not being used anymore\n",
    "                    os.remove(output_tiff)\n",
    "\n",
    "                # Remove column 'geometry'\n",
    "                zone_data = zone_data.drop(columns=['geometry'])\n",
    "\n",
    "                # Take out only the projection data\n",
    "                zone_data_projection = zone_data.loc[cutoff_year*num_rows:]\n",
    "\n",
    "                # Create path and name of projection .CSV table\n",
    "                output_table_projection = os.path.join(output_proc, raster_name + '.csv')\n",
    "\n",
    "                # Export dataframe as .CSV file\n",
    "                zone_data_projection.to_csv(output_table_projection, index=False)\n",
    "\n",
    "                # Take out only the historical data\n",
    "                zone_data_historical = zone_data.loc[:cutoff_year*num_rows-1]\n",
    "                \n",
    "                # Create path and name of historical .CSV table\n",
    "                output_table_historical = os.path.join(output_proc, raster_name.replace('_ssp245','_historical').replace('_ssp370','_historical').replace('_ssp585','_historical') + '.csv')\n",
    "\n",
    "                # Export dataframe as .CSV file\n",
    "                zone_data_historical.to_csv(output_table_historical, index=False)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.info(\"Process encountered an error while processing {netcdf}:\\n{str(e)}\\n\"\\\n",
    "                        f\"{traceback.format_exc()}\")\n",
    "                \n",
    "    # finally:\n",
    "    finish = time.perf_counter()\n",
    "    duration = round(finish-start, 3)\n",
    "    logging.info(\"Time to process rasters in %s:  %s secs\", netcdf, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-raf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
