{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    \"\"\"\n",
    "    Checks if directory exists and if not it will create it\n",
    "    \n",
    "    dir: str\n",
    "        full path the directory\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir):\n",
    "            os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_return_list_by_selection(workspace, selection):\n",
    "    \"\"\"\n",
    "    Lists all files in a workspace and return the full path\n",
    "\n",
    "    workspace: str\n",
    "        full path the directory\n",
    "    selection: str\n",
    "        string used as a filter \n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(workspace):\n",
    "        if selection in file:\n",
    "            files.append(os.path.join(workspace,file))\n",
    "    return(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_md_rasters_LOCA2(file_historical, file_1, out_file, variable):\n",
    "\n",
    "    try: \n",
    "        ds0 = xr.open_dataset(file_historical)\n",
    "        ds1 = xr.open_dataset(file_1)\n",
    "\n",
    "        # flip datasets upside down\n",
    "        ds0 = ds0.isel(lat=slice(None, None, -1))\n",
    "        ds1 = ds1.isel(lat=slice(None, None, -1))\n",
    "\n",
    "        merge_result = xr.merge([ds0,ds1])\n",
    "\n",
    "        # complevel 1 - 9. 1 is fastest and compresses least. 9 is slowest and compresses most\n",
    "        merge_result.to_netcdf(out_file, encoding={variable.replace('-', '_'): {'zlib': True, 'complevel': 9}})\n",
    "\n",
    "    # except starts if operation above fails\n",
    "    except Exception as err:\n",
    "\n",
    "        # logs info message if operations fails\n",
    "        logging.info(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_md_rasters_STAR(file_historical, file_1, file_2, out_file, variable):\n",
    "\n",
    "    try: \n",
    "        ds0 = xr.open_dataset(file_historical)\n",
    "        ds1 = xr.open_dataset(file_1)\n",
    "        ds2 = xr.open_dataset(file_2)\n",
    "\n",
    "        # flip datasets upside down\n",
    "        ds0 = ds0.isel(latitude=slice(None, None, -1))\n",
    "        ds1 = ds1.isel(latitude=slice(None, None, -1))\n",
    "        ds2 = ds2.isel(latitude=slice(None, None, -1))\n",
    "\n",
    "        merge_result = xr.merge([ds0,ds1,ds2])\n",
    "\n",
    "        # complevel 1 - 9. 1 is fastest and compresses least. 9 is slowest and compresses most\n",
    "        merge_result.to_netcdf(out_file, encoding={variable.replace('-', '_'): {'zlib': True, 'complevel': 9}})\n",
    "\n",
    "    # except starts if operation above fails\n",
    "    except Exception as err:\n",
    "\n",
    "        # logs info message if operations fails\n",
    "        logging.info(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Logger and Open Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your base directory\n",
    "base = r'C:\\<your-base-path>\\CRIS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_file_name = \"Output_Log_\" + file_time + \".log\"\n",
    "\n",
    "LOG_FILE_PATH = os.path.join(base, 'logs')\n",
    "LOG_FILE = os.path.join(LOG_FILE_PATH, log_file_name)\n",
    "\n",
    "logging.basicConfig(level = logging.INFO,\n",
    "                    format=\"%(asctime)s:%(levelname)s: %(message)s\",\n",
    "                    handlers=[\n",
    "                       logging.FileHandler(filename=LOG_FILE),\n",
    "                       logging.StreamHandler()\n",
    "                   ])\n",
    "logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data that will be processed needs to be downloaded into <base>\\data\\download\\model_set\\variable\\model\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =  os.path.join(base, 'data', 'download')\n",
    "output_dir = os.path.join(base, 'data', 'merge')\n",
    "\n",
    "create_dir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick STAR or LOCA2\n",
    "model_set = 'LOCA2'\n",
    "\n",
    "# pick model\n",
    "model = 'ACCESS-CM2'\n",
    "\n",
    "# pick variable\n",
    "variable = 'tavg'\n",
    "\n",
    "if model_set == 'STAR':\n",
    "    ssp_list = ['_ssp245','_ssp585']\n",
    "elif model_set == 'LOCA2':\n",
    "    ssp_list = ['_ssp245','_ssp370','_ssp585']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_proc_tm = time.time() \n",
    "\n",
    "netcdf_dir_list = os.path.join(input_dir, model_set, variable, model)\n",
    "\n",
    "# Get a list of NetCDFs from the downloaded directory\n",
    "netcdf_list = list_dir_return_list_by_selection(netcdf_dir_list, '')\n",
    "\n",
    "# Strip xmls from the list\n",
    "netcdf_list = list(set([str(item).strip(\".xml\").strip('.au') for item in netcdf_list]))\n",
    "\n",
    "# Define the full directory path\n",
    "output_dir_model_set = os.path.join(output_dir, model_set)\n",
    "\n",
    "# Create an output folder for the model set\n",
    "create_dir(output_dir_model_set)\n",
    "logging.info(f\"Created [{output_dir_model_set}] for processing\")\n",
    "\n",
    "# Define the full directory path\n",
    "output_dir_model_set_variable = os.path.join(output_dir_model_set, variable)\n",
    "\n",
    "# Create an output folder for the model variable\n",
    "create_dir(output_dir_model_set_variable)\n",
    "logging.info(f\"Created [{output_dir_model_set_variable}] for processing\")\n",
    "\n",
    "# Execute the workflow by ssp in a for loop\n",
    "for ssp in ssp_list:\n",
    "\n",
    "    ssp_rasters = list_dir_return_list_by_selection(netcdf_dir_list, ssp)\n",
    "\n",
    "    # Strip .xmls from the ssp_rasters list\n",
    "    ssp_rasters = list(set([str(item).strip(\".xml\") for item in ssp_rasters]))\n",
    "\n",
    "    start_rec_tm = time.time()\n",
    "    \n",
    "    # Split the file name by underscore and collect the parts for processing\n",
    "    file_name_parts = os.path.basename(ssp_rasters[0]).strip('.nc').split(\"_\")\n",
    "    \n",
    "    if model_set == 'STAR':\n",
    "\n",
    "        # Collect file names parts\n",
    "        model, variable, ssp, file, start_year, end_year = file_name_parts\n",
    "        logging.info(f\"Processing {os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp]))}\")\n",
    "\n",
    "        file_historical = os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp,'historical_1950_2014.nc']))\n",
    "        file_1 = os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp,'1_2015_2058.nc']))\n",
    "        file_2 = os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp,'2_2059_2100.nc']))\n",
    "\n",
    "        # Define the output NetCDF filename and path\n",
    "        out_netcdf = os.path.join(output_dir_model_set_variable, '_'.join([model_set, model, variable, ssp, '1950_2100.nc'])) \n",
    "        \n",
    "        # Merge multidimensional rasters \n",
    "        merge_md_rasters_STAR(file_historical, file_1, file_2, out_netcdf, variable)\n",
    "\n",
    "        end_red_tm = time.time()\n",
    "        elapsed_rec_tm = end_red_tm - start_rec_tm\n",
    "        logging.info(f\"[Processed: {os.path.join(netcdf_dir_list, ':', ssp)} in {elapsed_rec_tm:0,.2f} seconds]\")\n",
    "\n",
    "    elif model_set == 'LOCA2':\n",
    "\n",
    "        # Collect file names parts\n",
    "        model, variable, ssp, start_year, end_year = file_name_parts\n",
    "        logging.info(f\"Processing {os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp]))}\")\n",
    "\n",
    "        # Define the matching historical file\n",
    "        file_historical = os.path.join(netcdf_dir_list, '_'.join([model,variable,'historical_1950_2014.nc']))\n",
    "        file_1 = os.path.join(netcdf_dir_list, '_'.join([model,variable,ssp,'2015_2100.nc']))\n",
    "        \n",
    "        # Define the output NetCDF filename and path\n",
    "        out_netcdf = os.path.join(output_dir_model_set_variable, '_'.join([model_set, model, variable, ssp, '1950_2100.nc'])) \n",
    "        \n",
    "        # Merge multidimensional rasters\n",
    "        merge_md_rasters_LOCA2(file_historical, file_1, out_netcdf, variable)\n",
    "\n",
    "        end_red_tm = time.time()\n",
    "        elapsed_rec_tm = end_red_tm - start_rec_tm\n",
    "        logging.info(f\"[Processed: {os.path.join(netcdf_dir_list, ':', ssp)} in {elapsed_rec_tm:0,.2f} seconds]\")\n",
    "        \n",
    "logging.info(\"Processing complete.\")\n",
    "end_proc_tm = time.time()\n",
    "elapsed_proc_tm = end_proc_tm - start_proc_tm\n",
    "\n",
    "logging.info(f\"[Overall Elapsed time: {elapsed_proc_tm:0,.2f} seconds]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
