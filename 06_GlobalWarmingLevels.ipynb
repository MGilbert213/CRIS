{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir):\n",
    "    \"\"\"\n",
    "    Checks if directory exists and if not it will create it\n",
    "\n",
    "    dir: str\n",
    "        full path the directory\n",
    "    \"\"\"\n",
    "    path = Path(dir)\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_return_list_by_selection(workspace, selection):\n",
    "    \"\"\"\n",
    "    Lists all files in a workspace and return the full path\n",
    "\n",
    "    workspace: str\n",
    "        full path the directory\n",
    "    selection: str\n",
    "        string used as a filter \n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in Path(workspace).iterdir():\n",
    "        if selection in file.name:\n",
    "            files.append(file.resolve())\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_xml(file_list):\n",
    "    \"\"\"\n",
    "    Removes files from the list that end with '.csv'.\n",
    "\n",
    "    file_list: list\n",
    "        List of file paths as strings.\n",
    "    \"\"\"\n",
    "    # Convert strings to Path objects\n",
    "    paths = [Path(file) for file in file_list]\n",
    "    \n",
    "    # Filter out files that end with '.csv'\n",
    "    filtered_files = [file for file in paths if not file.name.endswith('.xml')]\n",
    "    \n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surpress warning due to adding model and gwl to dataframe\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Global Warming Level years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GWLS = [\"15\", \"20\", \"30\", \"40\"]\n",
    "\n",
    "GWL_dict = {\"ACCESSCM2\":    {\"15\": 2025, \"20\": 2038, \"30\": 2055, \"40\": 2072},\n",
    "            \"ACCESSESM15\": {\"15\": 2027, \"20\": 2039, \"30\": 2060, \"40\": 2078},\n",
    "            \"BCCCSM2MR\":   {\"15\": 2030, \"20\": 2043, \"30\": 2065, \"40\": -888},\n",
    "            \"CanESM5\":       {\"15\": 2012, \"20\": 2022, \"30\": 2040, \"40\": 2054},\n",
    "            \"ECEarth3\":     {\"15\": 2024, \"20\": 2035, \"30\": 2057, \"40\": 2073},\n",
    "            \"FGOALSf3L\":   {\"15\": 2022, \"20\": 2037, \"30\": 2060, \"40\": 2079},\n",
    "            \"FGOALSg3\":     {\"15\": 2022, \"20\": 2037, \"30\": 2060, \"40\": 2079},\n",
    "            \"GFDLESM4\":     {\"15\": 2039, \"20\": 2052, \"30\": 2075, \"40\": -888},\n",
    "            \"INMCM48\":     {\"15\": 2030, \"20\": 2046, \"30\": 2069, \"40\": -888},\n",
    "            \"INMCM50\":     {\"15\": 2030, \"20\": 2046, \"30\": 2074, \"40\": -888},\n",
    "            \"IPSLCM6ALR\":  {\"15\": 2018, \"20\": 2034, \"30\": 2050, \"40\": 2066},\n",
    "            \"MIROC6\":        {\"15\": 2040, \"20\": 2053, \"30\": 2076, \"40\": -888},\n",
    "            \"MPIESM12HR\": {\"15\": 2033, \"20\": 2049, \"30\": 2073, \"40\": -888},\n",
    "            \"MPIESM12LR\": {\"15\": 2033, \"20\": 2049, \"30\": 2071, \"40\": 2091},\n",
    "            \"MRIESM20\":    {\"15\": 2026, \"20\": 2038, \"30\": 2064, \"40\": 2083}, \n",
    "            \"NorESM2LM\":    {\"15\": 2042, \"20\": 2056, \"30\": 2077, \"40\": -888},\n",
    "            \"NorESM2MM\":    {\"15\": 2039, \"20\": 2054, \"30\": 2076, \"40\": -888}\n",
    "}\n",
    "\n",
    "NOT_REACHED = -888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r'C:\\Users\\raf14049\\OneDrive - Esri\\Documents\\CRIS\\zonalstats\\out'\n",
    "output_dir = r'C:\\Users\\raf14049\\OneDrive - Esri\\Documents\\CRIS\\globalwarminglevels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick STAR or LOCA2\n",
    "model_set = 'STAR'\n",
    "\n",
    "# pick variable\n",
    "variable = 'cdd'\n",
    "\n",
    "# pick ssp scenario(s)\n",
    "ssp_list = ['_ssp245','_ssp370','_ssp585']\n",
    "\n",
    "# pick zone\n",
    "zone = 'TribalAreas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate climate state for global warming levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [C:\\Users\\raf14049\\OneDrive - Esri\\Documents\\CRIS\\globalwarminglevels\\STAR\\TribalAreas] for processing\n",
      "There are [2] files in SSP[585]\n",
      "Processing: STAR ACCESSCM2 cdd\n",
      "Global Warming Level: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raf14049\\AppData\\Local\\Temp\\1\\ipykernel_13912\\3862677062.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  climate_state_model = pd.concat([climate_state_model, climate_state_model_row])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Warming Level: 20\n",
      "Global Warming Level: 30\n",
      "Global Warming Level: 40\n",
      "Processing: STAR ACCESSESM15 cdd\n",
      "Global Warming Level: 15\n",
      "Global Warming Level: 20\n",
      "Global Warming Level: 30\n",
      "Global Warming Level: 40\n",
      "[Processed: C:\\Users\\raf14049\\OneDrive - Esri\\Documents\\CRIS\\zonalstats\\out\\STAR\\TribalAreas\\:\\_ssp585 in 10.37 seconds]\n",
      "Processing complete.\n",
      "[Overall Elapsed time: 0.35 minutes]\n"
     ]
    }
   ],
   "source": [
    "start_proc_tm = time.time() \n",
    "\n",
    "# create path to CSV's\n",
    "csv_dir_list = Path(input_dir) / model_set / zone\n",
    "\n",
    "# Define and create the full output path\n",
    "output_dir_model_set_zone = Path(output_dir) / model_set / zone\n",
    "create_dir(output_dir_model_set_zone)\n",
    "print(f\"Created [{output_dir_model_set_zone}] for processing\")\n",
    "\n",
    "# Execute the workflow by ssp in a for loop\n",
    "for ssp in ssp_list:\n",
    "\n",
    "    # get list of CSV's\n",
    "    ssp_rasters = list_dir_return_list_by_selection(csv_dir_list, ssp)\n",
    "\n",
    "    # Strip .xmls from the ssp_rasters list\n",
    "    ssp_rasters = clean_list_xml(ssp_rasters)\n",
    "    \n",
    "    total_recs = len(ssp_rasters)\n",
    "    print(f\"There are [{total_recs}] files in SSP[{ssp.strip('_ssp')}]\")\n",
    "    rec_count = 0\n",
    "\n",
    "    start_rec_tm = time.time()\n",
    "    \n",
    "    # create an empty dataframe\n",
    "    climate_state_model_row = pd.DataFrame(columns=['GEOID', 'MODEL', 'GWL', 'COUNT', 'AREA', 'VARIABLE',  'MIN', 'MAX', 'MEAN'])\n",
    "    climate_state_model = pd.DataFrame(columns=['GEOID', 'MODEL', 'GWL', 'COUNT', 'AREA', 'VARIABLE', 'MIN', 'MAX', 'MEAN'])\n",
    "\n",
    "    # loop over models\n",
    "    for csv in ssp_rasters:\n",
    "\n",
    "        rec_count += 1\n",
    "        start_rec_tm = time.time()\n",
    "\n",
    "        # Split the file name by underscore and collect the parts for processing\n",
    "        file_name_parts = csv.name.strip('.csv').split('_')\n",
    "\n",
    "        # Collect file names parts\n",
    "        model_set, model, variable = file_name_parts[:3]\n",
    "        print('Processing:', model_set, model, variable)\n",
    "\n",
    "        # open .csv's with pathlib - use file_path.read_text()?\n",
    "        raster_hist = pd.read_csv(Path(csv.parent) / f\"{model_set}_{model}_{variable}_historical.csv\")\n",
    "        raster_ssp =  pd.read_csv(csv)\n",
    "        raster_all = pd.concat([raster_hist, raster_ssp], ignore_index=True)\n",
    "        \n",
    "        # loop over GWL years\n",
    "        for gwl in GWLS:\n",
    "            \n",
    "            print('Global Warming Level:', gwl)\n",
    "\n",
    "            # sort dataframe by geoid\n",
    "            raster_sorted = raster_all.sort_values(by=['GEOID', 'Standard Time']).reset_index(drop=True)\n",
    "\n",
    "            # lookup GWL midpoint year\n",
    "            gwl_year = int(GWL_dict.get(model).get(gwl))\n",
    "\n",
    "            if gwl_year == NOT_REACHED:\n",
    "                print(\"GWL not reached by {model_set}:{model}:{variable}\")\n",
    "                continue\n",
    "\n",
    "            for geoid in np.unique(raster_all['GEOID']):            \n",
    "                \n",
    "                # select all rows with geoid\n",
    "                raster_geoid = raster_all[raster_all['GEOID'] == geoid]\n",
    "\n",
    "                # select 20y period around gwl\n",
    "                raster_geoid_20y = raster_geoid[(raster_geoid['Standard Time'] >= gwl_year-9) & (raster_geoid['Standard Time'] <= gwl_year+10)]\n",
    "\n",
    "                # add model and gwl to columns\n",
    "                raster_geoid_20y.loc[:,'MODEL'] = model\n",
    "                raster_geoid_20y.loc[:,'GWL'] = gwl\n",
    "\n",
    "                # Group by GEOID and calculate the mean over the 20y period around the gwl year\n",
    "                climate_state_model_row = raster_geoid_20y.groupby('GEOID').agg(\n",
    "                                            MODEL = ('MODEL', lambda x: model),\n",
    "                                            GWL = ('GWL', lambda x: gwl),\n",
    "                                            COUNT = ('COUNT', 'first'),\n",
    "                                            AREA = ('AREA', 'first'),\n",
    "                                            VARIABLE = ('Variable', 'first'),\n",
    "                                            MIN = ('MIN', 'mean'),\n",
    "                                            MAX = ('MAX', 'mean'),\n",
    "                                            MEAN = ('MEAN', 'mean')\n",
    "                                           ).reset_index()\n",
    "                \n",
    "                # add each geoid to the larger dataframe\n",
    "                climate_state_model = pd.concat([climate_state_model, climate_state_model_row])\n",
    "    \n",
    "    # drop index column\n",
    "    climate_state_model = climate_state_model.reset_index().drop('index', axis=1)\n",
    "    \n",
    "    # Group by GWL, YEAR, GEOID, and VARIABLE, and calculate the average for MIN, MAX, and MEAN between models\n",
    "    climate_state = climate_state_model.groupby(['GEOID', 'GWL', 'COUNT', 'AREA', 'VARIABLE']).agg(\n",
    "                        MIN=('MIN', 'mean'),\n",
    "                        MAX=('MAX', 'mean'),\n",
    "                        MEAN=('MEAN', 'mean')\n",
    "                    ).reset_index()\n",
    "\n",
    "    end_red_tm = time.time()\n",
    "    elapsed_rec_tm = end_red_tm - start_rec_tm\n",
    "    print(f\"[Processed: {os.path.join(csv_dir_list, ':', ssp)} in {elapsed_rec_tm:0,.2f} seconds]\")\n",
    "\n",
    "    # path to save output to\n",
    "    out_raster_path = Path(output_dir) / model_set / zone / f\"{'_'.join([model_set, variable, ssp.strip('_')])}.csv\"\n",
    "\n",
    "    # save the climate state (averaged over models) for each ssp as csv\n",
    "    climate_state.to_csv(out_raster_path, index=False, header=True)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "end_proc_tm = time.time()\n",
    "\n",
    "elapsed_proc_tm = end_proc_tm - start_proc_tm\n",
    "\n",
    "print(f\"[Overall Elapsed time: {elapsed_proc_tm/60:0,.2f} minutes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
